import asyncio
import random
import time
import math
import torch
import distributed_training
import bittensor as bt
import numpy as np

from typing import Any, Dict, List, Tuple
from distributed_training.averaging.exceptions import AllReduceError, ModelStateError
from distributed_training.protocol import AllReduce
from distributed_training.data.dataset import DatasetLoader
from distributed_training.utils.dendrite import (
    async_dendrite_forward,
)
from distributed_training.utils.r2 import r2_download
from transformers import AutoModelForCausalLM


class AveragingHandler:
    """Handles averaging round and outer step for both validators and miners."""

    def __init__(
        self,
        model,
        optimizer,
        outer_optimizer,
        grad_averager,
        retry_limit,
        retry_delay,
        uid,
        local_batch_size_train,
        local_batch_size_train_effective,
        tokenizer,
        device,
        logger,
        parameters_list=None,
    ):
        self.model = model
        self.inner_optimizer = optimizer
        self.outer_optimizer = outer_optimizer
        self.grad_averager = grad_averager
        self.test_loss_loop = asyncio.new_event_loop()
        self.retry_limit = retry_limit
        self.retry_delay = retry_delay
        self.uid = uid
        self.local_batch_size_train = local_batch_size_train
        self.local_batch_size_train_effective = local_batch_size_train_effective
        self.tokenizer = tokenizer
        self.device = device
        self.number_of_local_steps = (
            self.local_batch_size_train_effective // self.local_batch_size_train
        )
        self.logger = logger
        self.parameters_list = parameters_list
        self.master = True

    def _get_weights_sample(self) -> List[float]:
        """Get a sample of model weights for validation."""
        p = list(self.model.parameters())[-2]
        if hasattr(p, "to_local"):  # sharded/DTensor
            return p.to_local()[-10:].tolist()
        return p.detach().flatten()[-10:].cpu().tolist()

    async def _validate_weight_update(
        self, initial_weights: List[float], block: int
    ) -> bool:
        """Validate model weight updates."""
        final_weights = self._get_weights_sample()
        self.logger.info(f"Final Weights Sample: {final_weights}")

        if final_weights == initial_weights:
            raise ModelStateError("Weights unchanged after update")

        if sum(np.isnan(final_weights)) > 1:
            raise ModelStateError("NaN values detected in weights after update")

        # TODO Re-introduce
        # if await self._test_model_loss(block):
        #     raise ModelStateError("NaN values detected in loss generated by new model")

    async def fetch_training_data(self, block):
        """Async function to fetch training data"""
        attempt = 0
        while attempt < self.retry_limit:
            try:
                pages = await DatasetLoader.next_pages(
                    offset=block,
                    n_pages=5,
                    seed=self.uid,
                )
                random.seed(self.uid)
                random.shuffle(pages)

                dataset = await DatasetLoader.create(
                    batch_size=4,
                    sequence_length=1024,
                    pages_info=pages,
                    tokenizer=self.tokenizer,
                )

                return dataset
            except Exception as e:
                self.logger.error(f"Error fetching training data: {str(e)}")
                attempt += 1
                self.logger.warning(
                    f"Failed to fetch data, retrying. Attempt {attempt}/{self.retry_limit}"
                )
                if attempt < self.retry_limit:
                    time.sleep(self.retry_delay * attempt)  # Wait before the next retry
                else:
                    self.logger.error(
                        "Maximum retry limit reached. Unable to fetch data."
                    )
                    raise

    async def _test_model_loss(self, block) -> bool:
        dataset = await self.fetch_training_data(block)
        for inputs, labels in dataset:
            inputs = inputs.to(self.device)
            with torch.autocast(device_type="cuda", dtype=torch.bfloat16):
                outputs = self.model(input_ids=inputs, labels=inputs)
                loss = outputs.loss / self.number_of_local_steps

        return math.isnan(loss.item())

    async def run_validator_allreduce(
        self,
        timeout: int,
        wallet,
        metagraph,
        peerids_to_uids,
        miner_uids,
        master,
        block,
        bandwidth=None,
        min_group_size: int = None,
        request_timeout: float = None,
        allreduce_timeout: float = None,
        next_chunk_timeout: float = None,
        min_matchmaking_time: float = None,
    ) -> Tuple[bool, Dict[str, Any]]:
        """
        Process allreduce specifically for validator.

        Returns:
            Tuple[bool, Dict[str, Any]]: (success, results)
            - success: True if allreduce completed successfully, False otherwise
            - results: Dictionary containing peers and bandwidth info if successful, empty dict if failed
        """
        query_tasks = []
        results = {}
        all_reduce_success_status = True
        initial_weights = None

        try:
            # Used for load balancing and scoring
            if bandwidth is not None:
                self.grad_averager.bandwidth = bandwidth["download"]

            self.logger.info(":wait: Starting Pseudo Gradient Averaging..")
            # Start gradient averaging without waiting
            start_ar_time = time.perf_counter()
            gradient_averaging_step = self.grad_averager.step(
                timeout=timeout,
                wait=False,
                gather=0,
                peerids_to_uids=peerids_to_uids,
            )

            if master:
                # Send AllReduce query to pause miner training and perform global sync
                self.logger.info(
                    ":wait: AllReduce Query Sent Out. Waiting for AllReduce to finish.."
                )
                await async_dendrite_forward(
                    wallet=wallet,
                    axons=[metagraph.axons[uid] for uid in miner_uids],
                    synapse=AllReduce(
                        completion=False,
                        min_group_size=min_group_size,
                        request_timeout=request_timeout,
                        allreduce_timeout=allreduce_timeout,
                        next_chunk_timeout=next_chunk_timeout,
                        min_matchmaking_time=min_matchmaking_time,
                        timeout=timeout,
                    ),
                    connection_limit=len(miner_uids),
                    timeout=timeout,
                )
                self.logger.info("AllReduce Query Responses Received..")

            start_time = time.perf_counter()

            while (gradient_averaging_step.done() is False) and (
                (time.perf_counter() - start_time) <= (timeout)
            ):
                await asyncio.sleep(1)

            if gradient_averaging_step.done():
                end_ar_time = time.perf_counter()

                self.logger.info(
                    ":white_heavy_check_mark: Finished Averaging Pseudo Gradients"
                )
                self.grad_averager.notify_used_averaged_gradients()

                (
                    gathered,
                    failed_peers,
                    participating_peers,
                    modes,
                    bandwidths,
                ) = gradient_averaging_step.result()

                initial_weights = self._get_weights_sample()
                self.logger.info(f"Initial Weights Sample: {initial_weights}")

                all_reduce_success_status = True
                results = {
                    "gathered": gathered,
                    "failed_peers": failed_peers,
                    "participating_peers": participating_peers,
                    "modes": modes,
                    "bandwidths": bandwidths,
                    "duration": end_ar_time - start_ar_time,
                }
            else:
                all_reduce_success_status = False

        except Exception as e:
            self.logger.error(f"Unexpected error during Averaging Process: {str(e)}")
            all_reduce_success_status = False

        finally:
            if gradient_averaging_step:
                gradient_averaging_step.cancel()
                self.logger.info("Gradient Step Cleaned Up")
            if all_reduce_success_status:
                self.logger.success("Averaging Round Finished Succesfully")
            return all_reduce_success_status, results, initial_weights

    def calculate_allreduce_scores(
        self,
        participating_peers: list,
        failed_peers: list,
        alive_uids: list,
        peerids_to_uids: dict,
        event: dict,
        metagraph,
        modes: list = None,
        bandwidths: list = None,
    ) -> dict:
        """
        Calculate scores based on AllReduce participation status, modes, and bandwidths.

        Args:
            participating_peers (list): List of peers that participated in AllReduce
            failed_peers (list): List of peers that failed during AllReduce
            peerids_to_uids (dict): Mapping of peer IDs to UIDs
            modes (list, optional): List of modes for each participating peer
            bandwidths (list, optional): List of bandwidths for each participating peer

        Returns:
            dict: Scores for each UID based on participation and optional mode/bandwidth
        """
        # Convert peer IDs to UIDs
        participating_uids = []
        uid_modes = {}
        uid_bandwidths = {}

        for idx, peer in enumerate(participating_peers):
            uid = peerids_to_uids.get(str(peer), "'''")
            participating_uids.append(uid)
            if modes is not None:
                uid_modes[uid] = modes[idx]
            if bandwidths is not None:
                uid_bandwidths[uid] = bandwidths[idx]

        failed_uids = [
            peerids_to_uids.get(str(failed_peer), "'''") for failed_peer in failed_peers
        ]

        # Calculate participation metrics
        successful_peers_count = len(participating_peers) - len(failed_peers)

        # Update event metrics
        event.update(
            {
                "failed_peers_count": len(failed_peers),
                "participating_peers_count": len(participating_peers),
                "successful_peers_count": successful_peers_count,
            }
        )

        # Find max bandwidth for normalization if bandwidths are provided
        if (
            bandwidths
            and [bandwidth for bandwidth in bandwidths if bandwidth is not None] != []
            and max([bandwidth for bandwidth in bandwidths if bandwidth is not None])
            != []
        ):
            max_bandwidth = max(
                [bandwidth for bandwidth in bandwidths if bandwidth is not None]
            )

        # Initialize scores dictionary
        scores = {}
        status_dict = {}
        for uid in range(metagraph.n):  # Assuming 256 UIDs in metagraph
            str_uid = str(uid)
            if uid in participating_uids and uid not in failed_uids:
                # Base score for successful participation
                base_score = 1.0
                final_score = base_score
                status = "SUCCESS"

                # Apply mode penalty if modes are provided
                if modes is not None and uid in uid_modes:
                    if uid_modes[uid] == "AveragingMode.CLIENT":
                        final_score = 0.0
                        status = "WRONG_MODE"

                # Apply bandwidth bonus if bandwidths are provided
                if (
                    bandwidths is not None
                    and uid in uid_bandwidths
                    and status != "WRONG_MODE"
                ):
                    if uid_bandwidths[uid] is None:
                        final_score = 0.0
                    else:
                        bandwidth_bonus = 0.5 * (uid_bandwidths[uid] / max_bandwidth)
                        final_score += bandwidth_bonus
                        self.logger.info(
                            f"UID {uid} score breakdown - Base: {base_score:.2f}, Bandwidth bonus: {bandwidth_bonus:.2f}"
                        )

                scores[str_uid] = 1.0
                status_dict[str_uid] = status

            elif uid in failed_uids:
                scores[str_uid] = 0.0
                status_dict[str_uid] = "FAIL"
            elif uid in alive_uids:
                # If UID is chosen but not participating, assign a score of 0
                scores[str_uid] = 1.0
                status_dict[str_uid] = "NON_PARTICIPATING"
            else:
                scores[str_uid] = 0.0
                status_dict[str_uid] = "NOT_ALIVE"

        # Create rewards tensor
        rewards = torch.tensor([reward for reward in scores.values()])

        # Log participation and scoring details
        self.logger.info(f"Failed UIDs: {failed_uids}")
        self.logger.info(f"Participating UIDs: {participating_uids}")
        if modes is not None:
            self.logger.info(f"Modes by UID: {uid_modes}")
        if bandwidths is not None:
            self.logger.info(f"Bandwidths by UID: {uid_bandwidths}")
        self.logger.info(f"AllReduce UID Scores: {scores}")
        self.logger.info(f"AllReduce UID Rewards: {rewards}")

        return (
            rewards,
            status_dict,
            event,
            successful_peers_count,
        )

    async def run_miner_allreduce(
        self,
        synapse,
        local_progress,
        start_time,
        block,
        bandwidth=None,
    ) -> distributed_training.protocol.AllReduce:
        """Process allreduce specifically for miner."""
        initial_weights = None
        try:
            # # Clip gradients
            # torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)

            # Used for load balancing and scoring
            if bandwidth is not None:
                self.grad_averager.bandwidth = bandwidth["download"]
            self.logger.info(":wait: Starting Pseudo Gradient Averaging..")
            gradient_averaging_step = self.grad_averager.step(
                timeout=(synapse.timeout - 20),
                wait=False,
                gather=local_progress.samples_accumulated,
            )

            while (gradient_averaging_step.done() is False) and (
                (time.perf_counter() - start_time) <= (synapse.timeout - 20)
            ):
                await asyncio.sleep(1)

            if gradient_averaging_step.done():
                self.logger.info(
                    ":white_heavy_check_mark: Finished Averaging Pseudo Gradients"
                )
                self.grad_averager.notify_used_averaged_gradients()
                self.logger.info("B: returned from notify_used_averaged_gradients")

                initial_weights = self._get_weights_sample()
                self.logger.info(f"Initial Weights Sample: {initial_weights}")

                synapse.completion = True
            else:
                synapse.completion = False

        except Exception as e:
            self.logger.error(f"Unexpected Error During Averaging Process: {str(e)}")
            synapse.completion = False
            raise AllReduceError(
                f"Unexpected Error During Averaging Process: {str(e)}"
            ) from e

        finally:
            if gradient_averaging_step:
                gradient_averaging_step.cancel()
                self.logger.info("Gradient Step Cleaned Up")
            if synapse.completion:
                self.logger.success("Averaging Round Finished Succesfully")
            return synapse, initial_weights

    # TODO Test if this is necissary and if it is make this FSDP compliant
    def update_main_param_after_outer_step(self):
        """Update the main parameters with the inner optimizer step"""
        opt_parameters = [
            param
            for group in self.inner_optimizer.param_groups
            for param in group["params"]
        ]
        for main_param, opt_param in zip(
            tuple(self.state_averager.main_parameters[i] for i in self.parameters_list),
            opt_parameters,
        ):
            main_param.data.copy_(opt_param.data, non_blocking=True)

    def reset_main_parameters(self, r2, model_name, prefix, use_cache, output_dir):
        """Reset the optimizer parameteres to the parameters at the start of the epoch"""
        try:
            if use_cache:
                _ = r2_download(
                    self,
                    r2=r2,
                    bucket=model_name,
                    key=f"{prefix}model.safetensors",
                    donwload_on_all_ranks=False,
                    run_on_all_ranks=False,
                    destination=output_dir,
                )
                _ = r2_download(
                    self,
                    r2=r2,
                    bucket=model_name,
                    key=f"{prefix}/config.json",
                    donwload_on_all_ranks=False,
                    run_on_all_ranks=False,
                    destination=output_dir,
                )
            main_parameters = AutoModelForCausalLM.from_pretrained(
                output_dir,  # directory containing model files
                trust_remote_code=False,
            )
            opt_parameters = [
                param
                for group in self.outer_optimizer.param_groups
                for param in group["params"]
            ]
            for main_param, opt_param in zip(
                tuple(main_parameters.parameters()), opt_parameters
            ):
                opt_param.data.copy_(main_param.data, non_blocking=True)
        except Exception as e:
            self.logger.info(f"Failed To Reset Optimizer Parameters With Error: {e}")
